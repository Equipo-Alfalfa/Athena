{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from modulos.utils import  labeler\n",
    "from modulos.model import split_data, token_data, create_model, performance\n",
    "import pandas as pd\n",
    "import modulos.limpieza as clean_data\n",
    "import modulos.load_data as ldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def split_data(df):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x = df['text'] \n",
    "    y = df['label'] \n",
    "    return train_test_split(x,y,test_size = 0.2,random_state=42)\n",
    "\n",
    "def token_data(texts, tokenizer, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf' \n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "def performance(y_true,y_pred):\n",
    "# y_pred son los valores predecidos mientras que y_true son los valores reales\n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return{\n",
    "        'accuracy':accuracy,\n",
    "        'recall':recall,\n",
    "        'f1':f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_model(inputs, num,  num_filters=128, kernel_size=5, pool_size=2):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='gelu', input_shape=inputs))\n",
    "    model.add(layers.MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv1D(num_filters * 2, kernel_size, activation='gelu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation = 'gelu'))\n",
    "    model.add(layers.Dense(num,activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(text):\n",
    "    categories = {\n",
    "        \"malware\": \"Malware\",\n",
    "        \"phishing\": \"Phishing\",\n",
    "        \"ransomware\": \"Ransomware\",\n",
    "        \"trojan\": \"Trojan\",\n",
    "        \"adware\": \"Adware\",\n",
    "        \"worm\": \"Worm\",\n",
    "        \"spyware\": \"Spyware\",\n",
    "        \"ddos\": \"DDoS\",\n",
    "        \"distributed denial of service\": \"DDoS\",\n",
    "        \"zero day\": \"Zero Days\",\n",
    "        \"data breach\": \"Data Breach\",\n",
    "        \"social engineering\": \"Social Engineering\"\n",
    "    }\n",
    "    lower_text = text.lower()\n",
    "    for keyword, category in categories.items():\n",
    "        if keyword in lower_text:\n",
    "            return category\n",
    "    return \"Other\" \n",
    "\n",
    "def labeler(df):\n",
    "    df[\"label\"] = df[\"text\"].apply(categorize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # LOAD\n",
    "    #df = clean_data.limpieza(ldata.ldata1(), ldata.ldata2(), ldata.ldata3())\n",
    "    df = pd.read_csv('./datasets/clean_data.csv')\n",
    "    print(\"data loaded\")\n",
    "    #LABEL\n",
    "    df = labeler(df) \n",
    "\n",
    "    # SPLIT\n",
    "    x_train, x_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    # TOKENIZE\n",
    "    train_texts = x_train.tolist()  \n",
    "    val_texts = x_test.tolist()  \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    train_encodings = token_data(train_texts, tokenizer)\n",
    "    val_encodings = token_data(val_texts,tokenizer)\n",
    "    \n",
    "    # Actually training the model\n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    y_test = pd.get_dummies(y_test).values\n",
    "    inputs = (train_encodings['input_ids'].shape[1], 1) \n",
    "    num = y_train.shape[1]\n",
    "    print(\"creando modelo\")\n",
    "    model = create_model(inputs, num)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_encodings['input_ids'],y_train,epochs=5, batch_size=32, validation_data=(val_encodings['input_ids'],y_test))\n",
    "#    y_pred =[] # reemplazar\n",
    "#    y_true = [] # reemplazar \n",
    "#    metrics = performance(y_true, y_pred)\n",
    "#    print(metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
